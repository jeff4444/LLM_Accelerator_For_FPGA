# FPGA-Based Hardware Accelerator for Large Language Models (LLMs)

> A senior design project focused on designing and implementing a hardware accelerator optimized for LLM inference, specifically targeting the Qwen 2.5-0.5B architecture.

**Advisor:** Dr. Hassan Salmani

---

## ğŸ‘¥ Team Members

- **Aayush Jha**
- **Jeff Mofor Allo**
- **Chase Adams**
- **Bishesh Adhikari**

---

## ğŸ“– Project Overview

This project aims to design and implement a hardware accelerator optimized for Large Language Models (LLMs), specifically targeting the **Qwen 2.5-0.5B** architecture. By leveraging the parallel processing capabilities of Field-Programmable Gate Arrays (FPGAs), we aim to improve energy efficiency and reduce latency for key inference operations compared to general-purpose CPUs.

### Development Approach

The project follows a **bottom-up approach**:

1. **Software Prototyping** - Re-implementing the LLM inference pipeline (Tokenizer â†’ Embedding â†’ Attention) in "barebones" Python to establish a Golden Model
2. **RTL Design** - Translating these logical blocks into synthesizable Verilog/SystemVerilog
3. **FPGA Deployment** - Mapping the design to the AUP-ZU3 (Zynq UltraScale+) platform
4. **System Integration** - Building an end-to-end demo with Keyboard Input and VGA/HDMI Output

---

## ğŸ—ï¸ System Architecture

The accelerator implements a standard **Transformer Decoder inference pipeline**, split into two distinct execution stages:

### 1. Prefill Stage (Compute Bound)

- **Input:** Full user prompt (sequence of tokens)
- **Operation:** Parallel processing of all input tokens to generate the initial KV Cache and the first new token
- **Key Modules:**
  - Tokenizer (Host)
  - Embedding Lookup
  - Parallel Matrix Multiplication (GEMM)
  - Softmax

### 2. Decode Stage (Memory Bound)

- **Input:** Single generated token
- **Operation:** Auto-regressive generation of one token at a time using the cached Key/Value matrices
- **Key Modules:**
  - KV Cache Manager
  - Vector-Matrix Multiplication
  - Sampling (Top-K/Top-P)

---

## ğŸ“‚ Repository Structure

```
â”œâ”€â”€ hardware/               # (Future) Verilog/VHDL RTL source code
â”‚   â”œâ”€â”€ src/                # Core modules (Attention, FFN, Softmax)
â”‚   â”œâ”€â”€ sim/                # Testbenches and simulation waveforms
â”‚   â””â”€â”€ synthesis/          # Vivado/Quartus project files
â”‚
â”œâ”€â”€ software/               # Python Golden Models & Host Control
â”‚   â”œâ”€â”€ tokenizer.py        # BPE Tokenizer implementation (Logic verification)
â”‚   â”œâ”€â”€ embedding.py        # Weight extraction and embedding generation
â”‚   â”œâ”€â”€ self_attention.py   # Multi-head self-attention with GQA
â”‚   â””â”€â”€ inference.py        # (Future) Main loop (Prefill + Decode logic)
â”‚
â”œâ”€â”€ docs/                   # Design specifications and timeline
â”œâ”€â”€ Qwen2.5-0.5B/          # Model files (config.json, model.safetensors, etc.)
â””â”€â”€ README.md              # This file
```

---

## ğŸ› ï¸ Software Prototype (Golden Model)

Before moving to hardware, we are validating the logic using "barebones" Python scripts that mimic the hardware data flow.

### 1. Tokenizer (`tokenizer.py`)

Implements the **Byte Pair Encoding (BPE)** algorithm from scratch.

- **Input:** Raw text string (e.g., `"Hello world"`)
- **Data Sources:**
  - `merges.txt` - Merge priority rules
  - `vocab.json` - Token ID mapping
- **Output:** Integer list `[101, 204, ...]` ready for the hardware embedding layer

**Features:**
- Full BPE implementation without external dependencies
- Handles Qwen-specific tokenization rules
- Includes encode/decode functionality

### 2. Embedding Lookup (`embedding.py`)

Simulates the hardware memory read operation for the first layer.

- **Input:** Token IDs
- **Operation:** Fetches high-dimensional vectors (d=896) from the `.safetensors` weight file
- **Output:** Matrix Eâ° passed to the Attention Mechanism

**Features:**
- Direct weight extraction from safetensors format
- Configurable model directory paths
- Dimension verification against config.json

### 3. Self-Attention (`self_attention.py`)

Implements multi-head self-attention with Grouped Query Attention (GQA) using barebone FPGA-oriented operations.

- **Input:** Embedding vectors [Seq_Len, Hidden_Dim]
- **Architecture:** 
  - 14 Query attention heads
  - 2 Key-Value heads (GQA for efficiency)
  - 64-dimensional head size
- **Operations:**
  - RMS Normalization
  - Q, K, V projections
  - Scaled dot-product attention with causal masking
  - Multi-head output concatenation
- **Output:** Attention output [Seq_Len, Hidden_Dim]

**FPGA Hardware Mapping:**
- **MAC Units** - Vector dot products
- **Systolic Arrays** - Matrix-vector multiplication
- **Softmax Units** - Attention probability computation
- **Parallel Adders/Multipliers** - Element-wise operations

### 4. Integrated Pipeline (`integrated_pipeline.py`)

Demonstrates the complete prefill stage by chaining all modules together.

**Pipeline Flow:**
```
Text Input â†’ Tokenization (CPU) â†’ Embedding Lookup (CPU/Memory) â†’ Self-Attention (FPGA)
```

---

## ğŸš€ Getting Started

### Prerequisites

- **Python 3.8+**
- **torch** (for tensor verification)
- **safetensors** (for loading model weights)

### Installation

```bash
pip install torch safetensors
```

### Usage

#### Running the Tokenizer Prototype

```bash
python tokenizer.py
```

This will load the vocabulary and convert sample text into token IDs.

**Example:**
```bash
$ python tokenizer.py
Enter a string: Hello, how are you?
Input String: 'Hello, how are you?'
Output Tokens (IDs): [9906, 11, 527, 499, 366, 30]
Number of tokens: 6
```

#### Running the Embedding Lookup

```bash
python embedding.py
```

This demonstrates how integer IDs are converted into floating-point vectors using simulated weights.

**Example:**
```bash
$ python embedding.py
Enter a string to tokenize and embed: Hello world
Input String: 'Hello world'
Token IDs: [9906, 1917]
Number of tokens: 2
Shape: (2, 896)
(Sequence Length: 2, Hidden Dimension: 896)
```

#### Running Self-Attention

```bash
python self_attention.py
```

This demonstrates multi-head self-attention with GQA, processing embeddings through the attention mechanism.

**Example:**
```bash
$ python self_attention.py
Enter text to process through self-attention: Hello world
Input: 'Hello world'
Token IDs: [9906, 1917]
Embeddings shape: (2, 896)
âœ“ Self-attention complete. Output shape: [2][896]
```
---

## ğŸ“… Project Roadmap

| Phase | Focus | Key Deliverables |
|-------|-------|------------------|
| **Phase 1** | Research & Specs | Architecture definition, Toolchain setup, Python Golden Model |
| **Phase 2** | RTL Design | Verilog stubs for Embedding, QKV Projections, and Attention Core |
| **Phase 3** | Compute Implementation | Systolic Array/MAC units for Matrix Multiplication, Softmax Hardware |
| **Phase 4** | Pipeline Integration | Connecting Prefill & Decode stages, Implementing KV Cache logic |
| **Phase 5** | System I/O | Keyboard Controller (Input) and Display Driver (Output) integration |
| **Phase 6** | Verification | Co-simulation (FPGA vs. Python) and Performance Benchmarking |

---

## ğŸ“š References

- [Qwen 2.5 Technical Report](https://github.com/QwenLM/Qwen2.5)
- [Attention Is All You Need (Vaswani et al.)](https://arxiv.org/abs/1706.03762)
- Understanding LLM Prefill vs Decode

---

## ğŸ“ License

This project is part of a senior design course. Please refer to the model license in `Qwen2.5-0.5B/LICENSE` for model-specific licensing information.

---

## ğŸ¤ Contributing

This is a senior design project. For questions or collaboration, please contact the team members or advisor.

---

**Status:** ğŸš§ In Active Development - Phase 1 (Software Prototyping)

**Latest Progress:**
- âœ… Tokenizer (BPE) implementation complete
- âœ… Embedding layer complete
- âœ… Self-Attention with Grouped Query Attention (GQA) complete
- ğŸš§ Next: Feed-Forward Network (FFN) module
